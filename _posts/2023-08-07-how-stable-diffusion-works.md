---
layout: post
title:  "How Stable Diffusion Works"
date: 2023-08-07
image: assets/images/alien_desert.png
tags: [ AI ]
---



**ARTICLES**   

- Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. In <em>CVPR</em>, 2023. [pdf](https://arxiv.org/pdf/2307.01952.pdf){:target="_blank"} / [code](https://github.com/Stability-AI/generative-models){:target="_blank"} / [Model weights](https://huggingface.co/stabilityai/){:target="_blank"}
- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 10684–10695, June 2022. [pdf](https://arxiv.org/pdf/2112.10752.pdf){:target="_blank"} / [project page](https://ommer-lab.com/research/latent-diffusion-models/){:target="_blank"}   
- Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution.  In <em>NeurIPS</em>, 2019. [pdf](https://arxiv.org/pdf/1907.05600.pdf){:target="_blank"}
- Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models. In <em>NeurIPS</em>, 2020. [pdf](https://arxiv.org/pdf/2006.09011.pdf){:target="_blank"}
- Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. <em>arxiv Preprint arxiv:2006.11239</em>, 2020. [pdf](https://arxiv.org/pdf/2006.11239.pdf){:target="_blank"}
- Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical
Image Segmentation. In <em>MICCAI (3), volume 9351 of Lecture Notes in Computer Science</em>, pages 234–241. Springer, 2015. [pdf](https://arxiv.org/pdf/1505.04597.pdf){:target="_blank"}
- Kevin Frans,Lisa B.Soros, and Olaf Witkowski. Clipdraw. Exploring text-to-drawing synthesis through language- image encoders. In <em>ArXiv</em>, abs/2106.14843, 2021. [pdf](https://arxiv.org/pdf/2106.14843.pdf){:target="_blank"}
- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In <em>CVPR</em>, 2021. [pdf](https://arxiv.org/pdf/2103.00020.pdf){:target="_blank"}
- Jonathan Ho, and Tim Salimans. Classifier-Free Diffusion Guidance. In <em>NeurIPS</em>, 2022. [pdf](https://arxiv.org/pdf/2207.12598.pdf){:target="_blank"}
- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In <em>ICLR</em>, 2021. [pdf](https://arxiv.org/pdf/2010.11929.pdf){:target="_blank"}
- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In <em>NIPS</em>, pages 5998–6008, 2017. [pdf](https://arxiv.org/pdf/1706.03762.pdf){:target="_blank"}

- Lvmin Zhang and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. In <em>CVPR</em>, 2023. [pdf](https://arxiv.org/pdf/2302.05543.pdf){:target="_blank"}
- Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common Diffusion Noise Schedules and Sample Steps are Flawed. In <em>CVPR</em>, 2023. [pdf](https://arxiv.org/pdf/2305.08891.pdf){:target="_blank"}
- Tim Brooks, Aleksander Holynski, and Alexei A. Efros. InstructPix2Pix: Learning to Follow Image Editing Instructions. In <em>CVPR</em>, 2023. [pdf](https://arxiv.org/pdf/2211.09800.pdf){:target="_blank"}
- Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion. In <em>CVPR</em>, 2022. [pdf](https://arxiv.org/pdf/2209.14988.pdf){:target="_blank"}
- Jonathan Tseng, Castellon Rodrigo, and C. Karen Liu. Edge: Editable dance generation from music. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 448-458, 2023. [pdf](https://arxiv.org/pdf/2211.10658.pdf){:target="_blank"}
- Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust Large Mask Inpainting with Fourier Convolutions. In <em>CVPR</em>, 2022. [pdf](https://arxiv.org/pdf/2109.07161.pdf){:target="_blank"} / [project page](https://advimman.github.io/lama-project/){:target="_blank"}
- Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 22500-22510. 2023. [pdf](https://arxiv.org/pdf/2208.12242.pdf){:target="_blank"} / [project page](https://dreambooth.github.io/){:target="_blank"}
- Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. In <em>CVPR</em>, 2022. [pdf](https://arxiv.org/pdf/2208.01618.pdf){:target="_blank"}

**CODE**

The development of Stable Diffusion was funded and shaped by <em>**Stability AI**</em>. The model was released by the CompVis group at Ludwig Maximilian University of Munich. Development was led by Patrick Esser of Runway and Robin Rombach of CompVis, who were among the researchers who had earlier invented the latent diffusion model architecture used by Stable Diffusion. 

[CompVis / stable-diffusion](https://github.com/CompVis/stable-diffusion?ref=jousefmurad.com#stable-diffusion-v1){:target="_blank"}


**TRAINING DATA**

According to Wikipedia, Stable Diffusion was trained off three massive datasets collected by LAION, a nonprofit whose compute time was largely funded by <em>**Stability AI**</em>: laion2B-en, laion-high-resolution, and laion-aesthetics v2 5+, which are subsets of LAION-5B. The model was initially trained on the laion2B-en and laion-high-resolution subsets, with the last few rounds of training done on LAION-Aesthetics v2 5+, a subset of 600 million captioned images which the LAION-Aesthetics Predictor V2 predicted that humans would, on average, give a score of at least 5 out of 10 when asked to rate how much they liked them. The LAION-Aesthetics v2 5+ subset also excluded low-resolution images and images which LAION-5B-WatermarkDetection identified as carrying a watermark with greater than 80% probability. Final rounds of training additionally dropped 10% of text conditioning to improve Classifier-Free Diffusion Guidance. The model was trained using 256 Nvidia A100 GPUs on Amazon Web Services for a total of 150,000 GPU-hours, at a cost of $600,000.

**DIFFUSERS**

[Diffusers](https://huggingface.co/docs/diffusers/index){:target="_blank"} is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you’re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. Our library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.





