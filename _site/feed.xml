<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>planet+AI</title>
    <description>The moon is beautiful, isn&apos;t it?</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 13 Sep 2023 13:25:22 -0400</pubDate>
    <lastBuildDate>Wed, 13 Sep 2023 13:25:22 -0400</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>Webinar on Oct 6: Mixed Traffic Control Via Reinforcement Learning</title>
        <description>&lt;p&gt;&lt;img width=&quot;500&quot; src=&quot;/assets/images/rl_li.png&quot; class=&quot;img-fluid&quot; alt=&quot;webinar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When:&lt;/strong&gt; Friday, Oct 6, 2023 4-5pm Eastern &lt;br /&gt;
&lt;strong&gt;Location:&lt;/strong&gt; Online &lt;br /&gt;
&lt;strong&gt;Registration Link:&lt;/strong&gt; &lt;a href=&quot;https://tennessee.zoom.us/meeting/register/tZMrfuGprDgrE9y3cO-nb0IhKJmdXsVJuxz9&quot; target=&quot;_blank&quot;&gt;https://tennessee.zoom.us/meeting/register/tZMrfuGprDgrE9y3cO-nb0IhKJmdXsVJuxz9&lt;/a&gt;   &lt;br /&gt;
&lt;strong&gt;Cost:&lt;/strong&gt; Free&lt;br /&gt;
&lt;strong&gt;Details:&lt;/strong&gt;  &lt;br /&gt;
On Friday, Oct 6, 4-5pm Eastern, Dr. Weizi Li will give us a talk on Mixed Traffic Control Via Reinforcement Learning. Modeling and managing mixed traffic, which encompasses both robot and human-driven vehicles, is an increasingly significant focus within the field of Intelligent Transportation Systems. In this talk, I will introduce our recent progress on controlling mixed traffic at complex and unsignalized intersections through multi-agent reinforcement learning. Furthermore, I will discuss our subsequent research endeavors and future directions projecting from transportation autonomy to urban design and planning. Dr. Weizi Li is an Assistant Professor of Electrical Engineering and Computer Science at UTK. He was a Michael Hammer Postdoctoral Fellow at MIT and he received his Ph.D. in Computer Science from the UNC Chapel Hill. His research interests include intelligent transportation systems, robotics, machine learning, and multi-agent simulation. He is passionate about using simulation and machine learning to design and develop next-gen cyber-physical infrastructure for urban environments. 
This event is sponsored by the National Science Foundation and also by TN Space Grant Consortium at UTK.
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 13 Sep 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2023/09/13/webinar-oct-6-mixed-traffic-control-via-reinforcement-learning/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/09/13/webinar-oct-6-mixed-traffic-control-via-reinforcement-learning/</guid>
        
        <category>events</category>
        
        
      </item>
    
      <item>
        <title>Webinar on Sep 16: A Tutorial on Uncertainty Quantification of Neural Networks</title>
        <description>&lt;p&gt;&lt;img width=&quot;500&quot; src=&quot;/assets/images/uq_zhang.png&quot; class=&quot;img-fluid&quot; alt=&quot;webinar&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When:&lt;/strong&gt; Saturday, September 16, 2023 8-9am Eastern &lt;br /&gt;
&lt;strong&gt;Location:&lt;/strong&gt; Online &lt;br /&gt;
&lt;strong&gt;Registration Link:&lt;/strong&gt; &lt;a href=&quot;https://tennessee.zoom.us/meeting/register/tZYtfu2urj0pGdSAVVlD4_voRzhmtxjTtgNV&quot; target=&quot;_blank&quot;&gt;https://tennessee.zoom.us/meeting/register/tZYtfu2urj0pGdSAVVlD4_voRzhmtxjTtgNV&lt;/a&gt;   &lt;br /&gt;
&lt;strong&gt;Cost:&lt;/strong&gt; Free&lt;br /&gt;
&lt;strong&gt;Details:&lt;/strong&gt;  &lt;br /&gt;
This coming Saturday, Sep 18 8-9am Eastern, Dr. Xiaoge Zhang, An Assistasnt Professor in the Department of Industrial and Systems Engineering at the Hong Kong Polytechnic University will give us a talk on Uncertainty Quantification of Neural Networks. This talk provides a holistic lens on emerging uncertainty quantification (UQ) methods for ML models with a particular focus on neural networks and gives a tutorial-style description of several state-of-the-art UQ methods: Gaussian process regression, Bayesian neural network, neural network ensemble, and deterministic UQ methods focusing on spectral-normalized neural Gaussian process (SNGP). Established upon the mathematical formulations, we subsequently examine the soundness of these UQ methods quantitatively and qualitatively (by a toy regression example) to examine their strengths and shortcomings from different dimensions. Based on the findings of the comparison, we exploit the advantages of SNGP in UQ and develop an uncertainty-aware deep neural network to detect the defects of steel wire rope. Computational experiments and comparisons with state-of-the-art models suggest that the principled uncertainty quantified by SNGP not only substantially enhances the prediction performance, but also provides an essential layer of protection for neural network against out-of-distribution data.&lt;/p&gt;

&lt;p&gt;This event is sponsored by the National Science Foundation and also by TN Space Grant Consortium at UTK.
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 11 Sep 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2023/09/11/webinar-sep-15-a-tutorial-on-uncertainty-quantification-of-neural-network/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/09/11/webinar-sep-15-a-tutorial-on-uncertainty-quantification-of-neural-network/</guid>
        
        <category>events</category>
        
        
      </item>
    
      <item>
        <title>Project Apollo Related Links</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://catalog.data.gov/dataset/lunar-sample-compendium&quot; target=&quot;_blank&quot;&gt;The Apollo Lunar Sample (Data.gov)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://curator.jsc.nasa.gov/lunar/index.cfm#&quot; target=&quot;_blank&quot;&gt;Lunar Rocks and Soils from Apollo Missions (NASA)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.lpi.usra.edu/lunar/samples/#catalogues&quot; target=&quot;_blank&quot;&gt;The Apollo Lunar Samples (LPI)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.lpi.usra.edu/lunar/samples/atlas/thin_sections/&quot; target=&quot;_blank&quot;&gt;The Apollo Thin Sections (LPI)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://curator.jsc.nasa.gov/education/lunar-thinsections.cfm&quot; target=&quot;_blank&quot;&gt;The Apollo Lunar Petrographic Thin Section Set (NASA)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ares.jsc.nasa.gov/astromaterials3d/apollo-lunar.htm&quot; target=&quot;_blank&quot;&gt;Astromaterials 3D Apollo Lunar Collection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nasa.gov/mission_pages/apollo/images.html&quot; target=&quot;_blank&quot;&gt;The Apollo Image Gallery (NASA)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.darts.isas.jaxa.jp/planet/project/selene/hdtv/index.html.en&quot; target=&quot;_blank&quot;&gt;Kaguya HDTV Data (JAXA)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pubs.usgs.gov/publication/pp1048&quot; target=&quot;_blank&quot;&gt;The Apollo Landing Site Geology (USGS)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.lpi.usra.edu/resources/apollo&quot; target=&quot;_blank&quot;&gt;The Apollo Image Atlas&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.lpi.usra.edu/resources/apollopanoramas/&quot; target=&quot;_blank&quot;&gt;Apollo Surface Panoramas&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.lpi.usra.edu/lunar/documents&quot; target=&quot;_blank&quot;&gt;The Apollo-Era Documents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nssdc.gsfc.nasa.gov/planetary/planets/moonpage.html&quot; target=&quot;_blank&quot;&gt;The Moon (NASA National Space Science Data Center)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://astrogeology.usgs.gov/search?pmi-target=moon&quot; target=&quot;_blank&quot;&gt;The Moon (USGS ASTROPEDIA)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://airandspace.si.edu/explore/topics/space/apollo-program&quot; target=&quot;_blank&quot;&gt;The Apollo Program (Smithsonian)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://history.nasa.gov/alsj&quot; target=&quot;_blank&quot;&gt;The Apollo Lunar Surface Journal&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://history.nasa.gov/afj&quot; target=&quot;_blank&quot;&gt;The Apollo Flight Journal&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://authors.library.caltech.edu/5456/1/hrst.mit.edu/hrs/apollo/public/index.html&quot; target=&quot;_blank&quot;&gt;The Apollo Guidance Computer (Caltech)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.mit.edu/digitalapollo&quot; target=&quot;_blank&quot;&gt;Digital Apollo by David A. Mindell&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sunburstandluminary.com/SLhome.html&quot; target=&quot;_blank&quot;&gt;An Apollo Memoir by Don Eyles&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.americasuncommonsense.com&quot; target=&quot;_blank&quot;&gt;Apollo 17 Astronaut Harrison Schmitt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The thumbnail image of this article is &lt;em&gt;Apollo 17 Astronaut Harrison Schmitt Collects Lunar Rock Samples&lt;/em&gt; published by NASA/Marshall Space Flight Center. In this Apollo 17 onboard photo, Lunar Module pilot Harrison H. Schmitt collects rock samples from a huge boulder near the Valley of Tourus-Littrow on the lunar surface. The seventh and last manned lunar landing and return to Earth mission, the Apollo 17, carrying a crew of three astronauts: Schmitt; Mission Commander Eugene A. Cernan; and Command Module pilot Ronald E. Evans, lifted off on December 7, 1972 from the Kennedy Space Flight Center (KSC). Scientific objectives of the Apollo 17 mission included geological surveying and sampling of materials and surface features in a preselected area of the Taurus-Littrow region, deploying and activating surface experiments, and conducting in-flight experiments and photographic tasks during lunar orbit and transearth coast (TEC). These objectives included: Deployed experiments such as the Apollo lunar surface experiment package (ALSEP) with a Heat Flow experiment, Lunar seismic profiling (LSP), Lunar surface gravimeter (LSG), Lunar atmospheric composition experiment (LACE) and Lunar ejecta and meteorites (LEAM). The mission also included Lunar Sampling and Lunar orbital experiments. Biomedical experiments included the Biostack II Experiment and the BIOCORE experiment. The mission marked the longest Apollo mission, 504 hours, and the longest lunar surface stay time, 75 hours, which allowed the astronauts to conduct an extensive geological investigation. They collected 257 pounds (117 kilograms) of lunar samples with the use of the Marshall Space Flight Center designed Lunar Roving Vehicle (LRV). The mission ended on December 19, 1972.&lt;/p&gt;
</description>
        <pubDate>Wed, 09 Aug 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2023/08/09/project-apollo-related_links/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/08/09/project-apollo-related_links/</guid>
        
        <category>planet</category>
        
        <category>projectApollo</category>
        
        
      </item>
    
      <item>
        <title>Using Stable Diffusion WebUI on Google Colab</title>
        <description>&lt;p&gt;&lt;strong&gt;How to run Stable Diffusion WebUI on Google Colab&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Enable GPU.&lt;/li&gt;
  &lt;li&gt;Download Stable Diffusion WebUI: &lt;br /&gt;
 &lt;code&gt;!git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Go to the Stable Diffusion WebUI directory: &lt;br /&gt;
 &lt;code&gt;%cd /content/stable-diffusion-webui&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Launch the Stable Diffusion WebUI: &lt;br /&gt;
 &lt;code&gt;!python ./launch.py --share --xformers --enable-insecure-extension-access&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;Parameters:   &lt;br /&gt;
 &lt;code&gt;--share&lt;/code&gt;: to publish to gradio.app &lt;br /&gt;
 &lt;code&gt;--xformers&lt;/code&gt;: to improve image generation speed &lt;br /&gt;
 &lt;code&gt;--enable-insecure-extensions-access&lt;/code&gt;: to install extensions&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;More resources&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Here’s where you can find more about GIT:  &lt;br /&gt;
 &lt;a href=&quot;https://git-scm.com/docs/git&quot; target=&quot;_blank&quot;&gt;https://git-scm.com/docs/git&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Here’s where you can find Automatic 1111’s Stable Diffusion WebUI Github page: &lt;br /&gt;
 &lt;a href=&quot;https://github.com/AUTOMATIC1111/stable-diffusion-webui&quot; target=&quot;_blank&quot;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Here’s where you can find Automatic 1111’s Stable Diffusion WebUI wiki: &lt;br /&gt;
 &lt;a href=&quot;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki&quot; target=&quot;_blank&quot;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Heres’ where you can find more about how to use Google Colab: &lt;br /&gt;
 &lt;a href=&quot;https://research.google.com/colaboratory/faq.html&quot; target=&quot;_blank&quot;&gt;https://research.google.com/colaboratory/faq.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Here’s where you can find more about how to use shell commands in Google Colab: &lt;br /&gt;
 &lt;a href=&quot;https://jakevdp.github.io/PythonDataScienceHandbook/01.05-ipython-and-shell-commands.html&quot; target=&quot;_blank&quot;&gt;https://jakevdp.github.io/PythonDataScienceHandbook/01.05-ipython-and-shell-commands.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 08 Aug 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2023/08/08/using-stable-diffusion-webui-on-google-colab/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/08/08/using-stable-diffusion-webui-on-google-colab/</guid>
        
        <category>AI</category>
        
        <category>tutorials</category>
        
        
      </item>
    
      <item>
        <title>Stable Diffusion and SDXL</title>
        <description>&lt;p&gt;&lt;strong&gt;ARTICLES&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. In &lt;em&gt;CVPR&lt;/em&gt;, 2023. &lt;a href=&quot;https://arxiv.org/pdf/2307.01952.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt; / &lt;a href=&quot;https://github.com/Stability-AI/generative-models&quot; target=&quot;_blank&quot;&gt;code&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/stabilityai/&quot; target=&quot;_blank&quot;&gt;Model weights&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, pages 10684–10695, June 2022. &lt;a href=&quot;https://arxiv.org/pdf/2112.10752.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt; / &lt;a href=&quot;https://ommer-lab.com/research/latent-diffusion-models/&quot; target=&quot;_blank&quot;&gt;project page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution.  In &lt;em&gt;NeurIPS&lt;/em&gt;, 2019. &lt;a href=&quot;https://arxiv.org/pdf/1907.05600.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models. In &lt;em&gt;NeurIPS&lt;/em&gt;, 2020. &lt;a href=&quot;https://arxiv.org/pdf/2006.09011.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. &lt;em&gt;arxiv Preprint arxiv:2006.11239&lt;/em&gt;, 2020. &lt;a href=&quot;https://arxiv.org/pdf/2006.11239.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical
Image Segmentation. In &lt;em&gt;MICCAI (3), volume 9351 of Lecture Notes in Computer Science&lt;/em&gt;, pages 234–241. Springer, 2015. &lt;a href=&quot;https://arxiv.org/pdf/1505.04597.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Kevin Frans,Lisa B.Soros, and Olaf Witkowski. Clipdraw. Exploring text-to-drawing synthesis through language- image encoders. In &lt;em&gt;ArXiv&lt;/em&gt;, abs/2106.14843, 2021. &lt;a href=&quot;https://arxiv.org/pdf/2106.14843.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In &lt;em&gt;CVPR&lt;/em&gt;, 2021. &lt;a href=&quot;https://arxiv.org/pdf/2103.00020.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Jonathan Ho, and Tim Salimans. Classifier-Free Diffusion Guidance. In &lt;em&gt;NeurIPS&lt;/em&gt;, 2022. &lt;a href=&quot;https://arxiv.org/pdf/2207.12598.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In &lt;em&gt;ICLR&lt;/em&gt;, 2021. &lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In &lt;em&gt;NIPS&lt;/em&gt;, pages 5998–6008, 2017. &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Lvmin Zhang and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. In &lt;em&gt;CVPR&lt;/em&gt;, 2023. &lt;a href=&quot;https://arxiv.org/pdf/2302.05543.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common Diffusion Noise Schedules and Sample Steps are Flawed. In &lt;em&gt;CVPR&lt;/em&gt;, 2023. &lt;a href=&quot;https://arxiv.org/pdf/2305.08891.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tim Brooks, Aleksander Holynski, and Alexei A. Efros. InstructPix2Pix: Learning to Follow Image Editing Instructions. In &lt;em&gt;CVPR&lt;/em&gt;, 2023. &lt;a href=&quot;https://arxiv.org/pdf/2211.09800.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion. In &lt;em&gt;CVPR&lt;/em&gt;, 2022. &lt;a href=&quot;https://arxiv.org/pdf/2209.14988.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Jonathan Tseng, Castellon Rodrigo, and C. Karen Liu. Edge: Editable dance generation from music. In &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;, pp. 448-458, 2023. &lt;a href=&quot;https://arxiv.org/pdf/2211.10658.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust Large Mask Inpainting with Fourier Convolutions. In &lt;em&gt;CVPR&lt;/em&gt;, 2022. &lt;a href=&quot;https://arxiv.org/pdf/2109.07161.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt; / &lt;a href=&quot;https://advimman.github.io/lama-project/&quot; target=&quot;_blank&quot;&gt;project page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;, pp. 22500-22510. 2023. &lt;a href=&quot;https://arxiv.org/pdf/2208.12242.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt; / &lt;a href=&quot;https://dreambooth.github.io/&quot; target=&quot;_blank&quot;&gt;project page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. In &lt;em&gt;CVPR&lt;/em&gt;, 2022. &lt;a href=&quot;https://arxiv.org/pdf/2208.01618.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 08 Aug 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2023/08/08/stable-diffusion-and-SDXL/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/08/08/stable-diffusion-and-SDXL/</guid>
        
        <category>AI</category>
        
        
      </item>
    
      <item>
        <title>Google Colab Tutorial</title>
        <description>&lt;p&gt;Google Colab is a free Jupyter notebook environment that runs in the cloud. You can use Google Colab from your browser so you don’t have to install anything on your own computer. With Google Colab, you can access Google’s computing resources, write and execture code, save and share.&lt;/p&gt;

&lt;h1 id=&quot;use-the-bash-commands&quot;&gt;Use the Bash Commands&lt;/h1&gt;
&lt;p&gt;You can use most of the bash commands with a &lt;code&gt;!&lt;/code&gt; added in front of the command.&lt;/p&gt;
&lt;h1 id=&quot;use-gpus-on-colab&quot;&gt;Use GPUs on Colab&lt;/h1&gt;
&lt;p&gt;If you use the free GPU, click and choose &lt;code&gt;Runtime&lt;/code&gt; -&amp;gt; &lt;code&gt;Change Runtime Type&lt;/code&gt; -&amp;gt; &lt;code&gt;Hardware accelerator&lt;/code&gt; &lt;br /&gt;
If you use the paid version of the GPUs, click and choose &lt;code&gt;Runtime&lt;/code&gt; -&amp;gt; &lt;code&gt;Change Runtime Type&lt;/code&gt;,then you will choose the following:
&lt;img width=&quot;350&quot; src=&quot;/assets/images/gpu_colab.png&quot; class=&quot;img-fluid&quot; alt=&quot;Colab GPUs&quot; /&gt; &lt;br /&gt;
Click &lt;code&gt;Save&lt;/code&gt;.
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;ubuntu-20045-lts-at-the-time-of-this-writing&quot;&gt;Ubuntu 20.04.5 LTS at the time of this writing&lt;/h1&gt;
&lt;p&gt;The Colab notebook is running on top of the Ubuntu 20.04.5 LTS at the time of this writing (2/3/2023).
&lt;img width=&quot;700&quot; src=&quot;/assets/images/ubuntu.png&quot; class=&quot;img-fluid&quot; alt=&quot;Ubuntu&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;check-gpus&quot;&gt;Check GPUs&lt;/h1&gt;
&lt;p&gt;&lt;img width=&quot;700&quot; src=&quot;/assets/images/check_gpu.png&quot; class=&quot;img-fluid&quot; alt=&quot;GPUs&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;check-cuda-version-and-etc&quot;&gt;Check Cuda Version and etc.&lt;/h1&gt;
&lt;p&gt;&lt;img width=&quot;700&quot; src=&quot;/assets/images/check_version.png&quot; class=&quot;img-fluid&quot; alt=&quot;Check Cuda&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;install-python-39&quot;&gt;Install Python 3.9&lt;/h1&gt;
&lt;p&gt;&lt;img width=&quot;700&quot; src=&quot;/assets/images/install_python.png&quot; class=&quot;img-fluid&quot; alt=&quot;Install Python 3.9&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 08 Aug 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2023/08/08/google-colab-tutorial/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/08/08/google-colab-tutorial/</guid>
        
        <category>AI</category>
        
        <category>tutorials</category>
        
        
      </item>
    
      <item>
        <title>Experiment with AI on Space Topics Podcast Series</title>
        <description>&lt;iframe style=&quot;border-radius:12px&quot; src=&quot;https://open.spotify.com/embed/episode/7FikU0jhQgseLIvMIzji83?utm_source=generator&amp;amp;theme=0&quot; width=&quot;77%&quot; height=&quot;300&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; allow=&quot;autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture&quot; loading=&quot;lazy&quot;&gt;&lt;/iframe&gt;

&lt;iframe style=&quot;border-radius:12px&quot; src=&quot;https://open.spotify.com/embed/episode/4Vjc5RRvTmhGccPZHjYDlN?utm_source=generator&amp;amp;theme=0&quot; width=&quot;77%&quot; height=&quot;300&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; allow=&quot;autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture&quot; loading=&quot;lazy&quot;&gt;&lt;/iframe&gt;

</description>
        <pubDate>Mon, 07 Aug 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/2023/08/07/podcast-series/</link>
        <guid isPermaLink="true">http://localhost:4000/2023/08/07/podcast-series/</guid>
        
        <category>podcasts</category>
        
        
      </item>
    
  </channel>
</rss>
